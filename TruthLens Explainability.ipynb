{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dba8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2235b97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_features\n\u001b[0;32m     55\u001b[0m     }\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for test data\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: df\u001b[38;5;241m.\u001b[39mloc[test_indices, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_lemma_nostop\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m: df\u001b[38;5;241m.\u001b[39mloc[test_indices, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_count\u001b[39m\u001b[38;5;124m'\u001b[39m: df\u001b[38;5;241m.\u001b[39mloc[test_indices, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflesch_reading_ease\u001b[39m\u001b[38;5;124m'\u001b[39m: df\u001b[38;5;241m.\u001b[39mloc[test_indices, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflesch_reading_ease\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_label\u001b[39m\u001b[38;5;124m'\u001b[39m: y_test\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m: y_pred\n\u001b[0;32m     65\u001b[0m })\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Row predicted as Real (0)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m real_example \u001b[38;5;241m=\u001b[39m test_df[test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def explain_prediction(row, model, vectorizer, scaler, top_n=5):\n",
    "    \"\"\"\n",
    "        Explains the prediction of the model by showing the most influential features (both TF-IDF and readability metrics) for the prediction.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        row : pandas.Series\n",
    "            A row of the dataframe containing:\n",
    "              - 'text': the preprocessed text (e.g. from 'content_lemma_nostop')\n",
    "              - 'word_count': the document's word count\n",
    "              - 'sentence_count': the number of sentences\n",
    "              - 'flasch_reading_ease': the readability score\n",
    "        model : object\n",
    "            The trained machine learning model.\n",
    "        vectorizer : object\n",
    "            The TF-IDF vectorizer used to transform the text.\n",
    "        scaler : object\n",
    "            The scaler used for the readability metrics during training.\n",
    "        top_n : int\n",
    "            The number of features to display based on their contribution.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary with the prediction ('label') and the top contributing features ('features').\n",
    "    \"\"\"\n",
    "    #make sure ther eis text in the row\n",
    "    row['text'] = row['text'] if pd.notnull(row['text']) else ''\n",
    "    #transform the text using the vectorizer (this gets the first 5000 features)\n",
    "    tfidf_text = vectorizer.transform([row['text']])\n",
    "    #get the readability features from the row\n",
    "    readability_values = np.array([[row['word_count'], row['sentence_count'], row['flesch_reading_ease']]])\n",
    "    #scale the readability metrics using the same scaler as during training.\n",
    "    readability_scaled = scaler.transform(readability_values)\n",
    "    #convert the scaled features into a sparse matrix\n",
    "    readability_sparse = csr_matrix(readability_scaled)\n",
    "    #combine TF-IDF features and readability features\n",
    "    full_features = hstack([tfidf_text, readability_sparse])\n",
    "    # Predict the label.\n",
    "    prediction = model.predict(full_features)[0]\n",
    "    \n",
    "    #get contribution for each feature\n",
    "    contributions = full_features.toarray()[0]* model.coef_[0]\n",
    "    #get indices of features with highest absolute contributions\n",
    "    top_indices = np.argsort(np.abs(contributions))[-top_n:]\n",
    "    #get feature names\n",
    "    tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "    readability_feature_names = ['word_count', 'sentence_count', 'flesch_reading_ease']\n",
    "    all_feature_names = list(tfidf_feature_names) + readability_feature_names\n",
    "    top_features = [all_feature_names[i] for i in top_indices]\n",
    "\n",
    "    return {\n",
    "        \"label\": prediction,\n",
    "        \"features\": top_features\n",
    "    }\n",
    "\n",
    "# Create a DataFrame for test data\n",
    "test_df = pd.DataFrame({\n",
    "    'text': df.loc[test_indices, 'content_lemma_nostop'].reset_index(drop=True),\n",
    "    'word_count': df.loc[test_indices, 'word_count'].reset_index(drop=True),\n",
    "    'sentence_count': df.loc[test_indices, 'sentence_count'].reset_index(drop=True),\n",
    "    'flesch_reading_ease': df.loc[test_indices, 'flesch_reading_ease'].reset_index(drop=True),\n",
    "    'true_label': y_test.reset_index(drop=True),\n",
    "    'predicted_label': y_pred\n",
    "})\n",
    "\n",
    "# Row predicted as Real (0)\n",
    "real_example = test_df[test_df['predicted_label'] == 0].iloc[0]\n",
    "#print(real_example)\n",
    "\n",
    "# Row predicted as Fake (1)\n",
    "fake_example = test_df[test_df['predicted_label'] == 1].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_explanation = explain_prediction(real_example, final_model, vectorizer, scaler)\n",
    "fake_explanation = explain_prediction(fake_example, final_model, vectorizer, scaler)\n",
    "\n",
    "print(\"Real Example Prediction:\")\n",
    "print(\"Text:\", real_example['text'])\n",
    "print(\"Predicted Label:\", real_explanation['label'])\n",
    "print(\"Top Features:\", real_explanation['features'])\n",
    "\n",
    "print(\"\\nFake Example Prediction:\")\n",
    "print(\"Text:\", fake_example['text'])\n",
    "print(\"Predicted Label:\", fake_explanation['label'])\n",
    "print(\"Top Features:\", fake_explanation['features'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d38797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(texts):\n",
    "    \"\"\"\n",
    "    This function takes a list of raw text strings, vectorizes them using the\n",
    "    pre-fitted TF-IDF vectorizer, and returns the probability predictions\n",
    "    from the trained model.\n",
    "    \"\"\"\n",
    "    return model.predict_proba(vectorizer.transform(texts))\n",
    "\n",
    "# Create a LIME text explainer\n",
    "explainer = LimeTextExplainer(class_names=['true', 'fake'])\n",
    "\n",
    "# Choose an instance from the test set by its index\n",
    "idx = 0  # you can adjust this index as needed\n",
    "raw_text = df.loc[test_indices].iloc[idx]['content']\n",
    "\n",
    "# Use the custom predict_proba function in LIME\n",
    "exp = explainer.explain_instance(raw_text, predict_proba, num_features=10)\n",
    "exp.show_in_notebook(text=raw_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
