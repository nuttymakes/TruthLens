{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a806e51",
   "metadata": {},
   "source": [
    "# TruthLens Prototype\n",
    "\n",
    "### Objective of the Prototype\n",
    "\n",
    "The prototype should serve as a minimal viable product demonstrating:\n",
    "- A basic pipeline for text preprocessing, feature extraction, and classification.\n",
    "- The binary classification task (real vs. fake news) using a simple machine learning model.\n",
    "- The potential for explainability by outputting key features or tokens influencing the classification.\n",
    "\n",
    "This prototype is not expected to achieve high accuracy but should showcase the project’s core structure and provide a baseline for future development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623e577f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hazel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hazel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#get english stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73c8a4",
   "metadata": {},
   "source": [
    "## Phase 1: Binary Classification\n",
    "The ISOT database has been downloaded from https://onlineacademiccommunity.uvic.ca/isot/2022/11/27/fake-news-detection-datasets/ and the dataset description is available here: https://onlineacademiccommunity.uvic.ca/isot/wp-content/uploads/sites/7295/2023/02/ISOT_Fake_News_Dataset_ReadMe.pdf\n",
    "### Generate subset of ISOT dataset\n",
    "For the prototype, a subset of 1000 articles, 500 from each class, will be used. These 1000 articles are randomly selected and loaded into a dataframe with a class label (0 for True and 1 for False). The title and text fields are also concatenated into one column called \"content\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2e3b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    500\n",
      "0    500\n",
      "Name: label, dtype: int64\n",
      "                                                                                               content  \\\n",
      "0  WOW! BARBARA BUSH Will Be Keynote Speaker For Baby-Killing Planned Parenthood Fundraiser Her fat...   \n",
      "1  Bid to 'fix' Iran nuclear deal faces uphill climb in U.S. Congress WASHINGTON (Reuters) - Presid...   \n",
      "2  Suicide attack targets area southeast of Baghdad BAGHDAD (Reuters) - Two attackers shot several ...   \n",
      "3  Congressman Jim Jordan stops CNN gatekeeper Chris Cuomo on Benghazi cover-up  21st Century Wire ...   \n",
      "4  Islamic State claims responsibility for blast in Afghan capital, Kabul CAIRO (Reuters) - Islamic...   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      0  \n",
      "2      0  \n",
      "3      1  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "#Import the ISOT dataset\n",
    "ISOT_True_Full = 'ISOT-True.csv'  \n",
    "ISOT_True_Subset = 'ISOT-True-Subset.csv'\n",
    "\n",
    "#Set output files\n",
    "ISOT_Fake_Full = 'ISOT-Fake.csv'  \n",
    "ISOT_Fake_Subset = 'ISOT-Fake-Subset.csv'  \n",
    "\n",
    "#Set the sample amount\n",
    "sample_amount = 500\n",
    "\n",
    "\n",
    "def sample_csv(input_file, output_file, sample_amount):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, randomly selects a specified number of rows, and writes them to a new CSV file.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input CSV file.\n",
    "        output_file (str): Path to the output CSV file where the sampled rows will be saved.\n",
    "        sample_amount (int): Number of rows to randomly sample from the input file.\n",
    "\n",
    "    This function ensures that the CSV is read and written using UTF-8 encoding and handles encoding errors gracefully.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8', errors='replace') as csvfile:\n",
    "        reader = list(csv.reader(csvfile))\n",
    "        headers = reader[0]\n",
    "        data_rows = reader[1:]\n",
    "        sampled_rows = random.sample(data_rows, min(sample_amount, len(data_rows)))\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(sampled_rows)\n",
    "\n",
    "    #print(f\"Random sample of {len(sampled_rows)} rows written to {output_file}.\")\n",
    "\n",
    "# Generate a random sample from each class\n",
    "sample_csv(ISOT_True_Full, ISOT_True_Subset, sample_amount)\n",
    "sample_csv(ISOT_Fake_Full, ISOT_Fake_Subset, sample_amount)\n",
    "\n",
    "#load each subset to a dataframe\n",
    "true_df = pd.read_csv(ISOT_True_Subset)\n",
    "fake_df = pd.read_csv(ISOT_Fake_Subset)\n",
    "#add a label for each class - 0 for True and 1 for False\n",
    "true_df['label'] = 0\n",
    "fake_df['label'] = 1\n",
    "#combine the dataframes\n",
    "data = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "#shuffle the dataset\n",
    "data = data.sample(frac=1, random_state=999).reset_index(drop=True)\n",
    "#combine the title and text columns into a single column\n",
    "data['content'] = data['title'] + \" \" + data['text']\n",
    "#drop unnecessary columns\n",
    "data = data[['content', 'label']] \n",
    "#check we have 500 of each, and print the head\n",
    "print(data['label'].value_counts())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ea5da",
   "metadata": {},
   "source": [
    "### Replace censored words\n",
    "I've noticed that the data contains a lot of censored words - for example \"f*ck\" instead of \"fuck\", so I am replacing those with the uncensored versions before continuing with preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298e8c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Censored Words Found: ['batsh*t', 'p*ssing', 'f*ck', 'c*ck', 'apesh*t', 'n*gger', 'n*ggers', 'f*cking', 'sh*t']\n"
     ]
    }
   ],
   "source": [
    "def find_censored_words(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    Finds censored words in the specified column of a DataFrame. \n",
    "    Censored words typically include one or more asterisks (*) surrounded by other letters.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        The DataFrame containing the text data.\n",
    "    column_name : str\n",
    "        The name of the column to search for censored words.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list\n",
    "        A list of unique censored words found in the specified column.\n",
    "    \"\"\"\n",
    "    # Define the regex pattern for censored words (e.g., words with * surrounded by other letters)\n",
    "    pattern = r'\\b\\w*\\*\\w*\\b'\n",
    "    \n",
    "    # Combine all text in the specified column\n",
    "    combined_text = ' '.join(dataframe[column_name].astype(str))\n",
    "    \n",
    "    # Find all matches of the pattern\n",
    "    censored_words = re.findall(pattern, combined_text)\n",
    "    \n",
    "    # Return unique matches\n",
    "    return list(set(censored_words))\n",
    "\n",
    "# Example usage:\n",
    "data['content'] = data['content'].str.lower()\n",
    "censored_words = find_censored_words(data, 'content')\n",
    "print(\"Censored Words Found:\", censored_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ac96f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                               content  \\\n",
      "0  wow! barbara bush will be keynote speaker for baby-killing planned parenthood fundraiser her fat...   \n",
      "1  bid to 'fix' iran nuclear deal faces uphill climb in u.s. congress washington (reuters) - presid...   \n",
      "2  suicide attack targets area southeast of baghdad baghdad (reuters) - two attackers shot several ...   \n",
      "3  congressman jim jordan stops cnn gatekeeper chris cuomo on benghazi cover-up  21st century wire ...   \n",
      "4  islamic state claims responsibility for blast in afghan capital, kabul cairo (reuters) - islamic...   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      0  \n",
      "2      0  \n",
      "3      1  \n",
      "4      0  \n"
     ]
    }
   ],
   "source": [
    "substitutions = {\n",
    "    'b*tch': 'bitch',\n",
    "    'f*cks': 'fucks',\n",
    "    'd*mn': 'damn',\n",
    "    'sh*t': 'shit',\n",
    "    'f*ck': 'fuck',\n",
    "    'n*ggers': 'niggers',\n",
    "    'sh*tter': 'shitter',\n",
    "    'clusterf*ck': 'clusterfuck',\n",
    "    'f*cking': 'fucking',\n",
    "    'p*ssy':'pussy',\n",
    "    'sh*tty': 'shitty',\n",
    "    'motherf*cker': 'motherfucker',\n",
    "    'scr*wed': 'screwed',\n",
    "    'a*s': 'ass',\n",
    "    'h*ll': 'hell',\n",
    "    'sh*tshow': 'shitshow',\n",
    "    'f*ckin': 'fucking',\n",
    "    'bullsh*t': 'bullshit',\n",
    "    'p*ss': 'piss',\n",
    "    'p*ssies': 'pussies',\n",
    "    'f*cker': 'fucker',\n",
    "    'p*ssygrabber': 'pussygrabber',\n",
    "    'g*d': 'god',\n",
    "    'dumbf*ckery': 'dumbfuckery',\n",
    "    'sh*tfest': 'shitfest',\n",
    "    'f*cked': 'fucked',\n",
    "    'p*rn': 'porn',\n",
    "    'batsh*t': 'batshit',\n",
    "    'motherf*cking': 'motherfucking',\n",
    "}\n",
    "\n",
    "def replace_censored_words(text, substitutions):\n",
    "    \"\"\"\n",
    "    Replaces censored words in the given text based on the substitutions dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to process.\n",
    "    substitutions : dict\n",
    "        A dictionary where keys are censored words and values are their uncensored equivalents.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    str\n",
    "        The text with censored words replaced.\n",
    "    \"\"\"\n",
    "    # Replace each censored word in the text\n",
    "    for censored, uncensored in substitutions.items():\n",
    "        text = text.replace(censored, uncensored)\n",
    "    return text\n",
    "\n",
    "#replace the censored words\n",
    "data['content'] = data['content'].apply(lambda x: replace_censored_words(x, substitutions))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69fb5a",
   "metadata": {},
   "source": [
    "### Preprocess text\n",
    "In this step we do some basic NLP preprocessing - convert all text to lowercase and remove punctuation and stopwords. Then tokenise the text and return the processed text as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657a96ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                               content  \\\n",
      "0  wow! barbara bush will be keynote speaker for baby-killing planned parenthood fundraiser her fat...   \n",
      "1  bid to 'fix' iran nuclear deal faces uphill climb in u.s. congress washington (reuters) - presid...   \n",
      "2  suicide attack targets area southeast of baghdad baghdad (reuters) - two attackers shot several ...   \n",
      "3  congressman jim jordan stops cnn gatekeeper chris cuomo on benghazi cover-up  21st century wire ...   \n",
      "4  islamic state claims responsibility for blast in afghan capital, kabul cairo (reuters) - islamic...   \n",
      "\n",
      "                                                                                         clean_content  \n",
      "0  wow barbara bush keynote speaker babykilling planned parenthood fundraiser father staunch suppor...  \n",
      "1  bid fix iran nuclear deal faces uphill climb us congress washington reuters president donald tru...  \n",
      "2  suicide attack targets area southeast baghdad baghdad reuters two attackers shot several civilia...  \n",
      "3  congressman jim jordan stops cnn gatekeeper chris cuomo benghazi coverup 21st century wire says ...  \n",
      "4  islamic state claims responsibility blast afghan capital kabul cairo reuters islamic state claim...  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "        Preprocesses a given text string by applying the following steps:\n",
    "        1. Converts the text to lowercase.\n",
    "        2. Removes punctuation marks.\n",
    "        3. Tokenizes the text into individual words.\n",
    "        4. Removes stopwords (common words that add little value to classification tasks).\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        text : str\n",
    "            The input text string to preprocess.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The cleaned and preprocessed text, with tokens joined back into a single string.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "data['clean_content'] = data['content'].apply(preprocess_text)  # Adjust to 'content' if using combined column\n",
    "print(data[['content', 'clean_content']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581d511",
   "metadata": {},
   "source": [
    "### Feature Extraction Using TF-IDF and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e155a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(data['clean_content'])\n",
    "\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486dff35",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b0a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain the indices as we need these for looking up explanations later\n",
    "train_indices, test_indices = train_test_split(data.index, test_size=0.2, random_state=42)\n",
    "# Split X and y using the train/test indices\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_train = y.iloc[train_indices]\n",
    "y_test = y.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa836aa",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5125eb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.965\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       103\n",
      "           1       0.96      0.97      0.96        97\n",
      "\n",
      "    accuracy                           0.96       200\n",
      "   macro avg       0.96      0.97      0.96       200\n",
      "weighted avg       0.97      0.96      0.97       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d69e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_prediction(text):\n",
    "    \"\"\"\n",
    "    Explains the prediction of the model by showing the most influential words for the prediction.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text : str\n",
    "        The input text to analyze.\n",
    "    model : object\n",
    "        The trained machine learning model.\n",
    "    vectorizer : object\n",
    "        The TF-IDF vectorizer used to transform the text.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the prediction ('label') and the top contributing words ('features').\n",
    "    \"\"\"\n",
    "    # Transform the text using the vectorizer\n",
    "    tfidf_text = vectorizer.transform([text])\n",
    "    # Predict the label\n",
    "    prediction = model.predict(tfidf_text)[0]\n",
    "    # Get top contributing features (words)\n",
    "    feature_importances = model.coef_[0]  # Logistic regression coefficients\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    # Sort by importance\n",
    "    top_indices = tfidf_text.toarray().argsort()[0][-5:]  # Top 5 features\n",
    "    top_features = [feature_names[i] for i in top_indices]\n",
    "\n",
    "    return {\n",
    "        \"label\": prediction,\n",
    "        \"features\": top_features\n",
    "    }\n",
    "\n",
    "# Create a DataFrame for test data\n",
    "test_df = pd.DataFrame({\n",
    "    'text': data.loc[test_indices, 'clean_content'].reset_index(drop=True),\n",
    "    'true_label': y_test.reset_index(drop=True),\n",
    "    'predicted_label': y_pred\n",
    "})\n",
    "\n",
    "# Row predicted as Real (0)\n",
    "real_example = test_df[test_df['predicted_label'] == 0].iloc[0]\n",
    "\n",
    "# Row predicted as Fake (1)\n",
    "fake_example = test_df[test_df['predicted_label'] == 1].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "369074ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Example Prediction:\n",
      "Text: factbox new zealand 2017 election main parties policies reuters new zealand two main parties neck neck opinion polls appointment charismatic leader boosted opposition labour party threatening governing national party decadelong hold power election sept 23 main parties positions key issues economynew zealand booming economy facing capacity constraints unemployment eightyear low labor shortage noticeably construction threatens curb growth main parties plan fiscally prudent maintain budget surplus would differ monetary policy national plans cut net debt 1015 percent gdp 2025 labour green party plan cut 20 percent gdp within five years taking office labour proposes adding full employment existing inflation mandate reserve bank new zealand economists say could lead easier monetary policy fuel longerterm inflation form government national may dependent new zealand first favors greater currency intervention something new zealand reluctant past national planning adjust tax thresholds effectively deliver tax cuts april 2018 boost family income labour wants away national planned tax cuts boost tax credits subsidies new zealand house prices risen 50 percent past decade construction industry failed keep demand growing population fueled record migration increasing number new zealanders staying home labour criticizes national leaving housing crisis unresolved nine years government wants ban overseas buyers purchasing existing homes build 100000 affordable homes 10 years also plans create housing authority speed residential development national questions labour ability build many houses curbing immigration plans make nz1 billion available speed development 60000 houses new zealand first wants ban foreign nonresidents owning home new zealand except particular cases provide government assistance firsthome buyers green party wants provide 10000 new houses 10 years lowincome groups renttobuy scheme national party tightened eligibility criteria year immigration believes current levels immigration right meet economy needs labour plans reduce net immigration 30000 record levels 70000 annually charge every visitor nz25 fee would ringfenced nz75 million infrastructure fund greens want review immigration policy make sure migrants match skills employers need line labour policy new zealand first wants curb immigration ensuring new zealand workers first chance jobs capping number older migrants national wants expand new zealand international trade among plans wants complete transpacific partnership trade deal launch free trade deal european union britain postbrexit upgrade free trade agreement china labour wants reconsider new zealand role transpacific partnership trade pact provisions foreign investment changed allow new zealand government restrict investment country new zealand first wants renegotiate tpp\n",
      "Predicted Label: 0\n",
      "Top Features: ['wants', 'new', 'labour', 'new zealand', 'zealand']\n",
      "\n",
      "Fake Example Prediction:\n",
      "Text: lawyer fbi informant knows russian bribery info ‘that involves clintons’ video dc lawyer victoria toensing one smart cookie representing former fbi informant evidence kickbacks bribery involving transportation uranium us recently told sean hannity client brief congress russian involvement us uranium market includes widespread bribery actions involved clintons going detail attorney victoria toensing said oct 24 hannity know sean informant give overview specific conversations russians thinking money spending mean let general involves clintons director fbi time robert mueller special counsel investigating alleged russian collusion 2016 trump campaign undercover investigation involving toensing client occurred 2009 2014 senior attorney case rod rosenstein deputy attorney general united states official appointed mueller special counselfurther information indicates many senior obama administration officials knew instances bribery money laundering involving least one russian official time russia wanted expand uranium market united states administration special committee approve deny sale company vancouverbased uranium one rosatom rosatom russian state atomic energy corporationsome people committee foreign investment united states included thensecretary state hillary clinton attorney general eric holder homeland security secretary janet napalitano treasury secretary timothy geithnerthe committee approved sale uranium one rosatom october 2010 sale gave russia president vladimir putin control 20 us uranium production least nine investors uranium one prior sale donated 145 million clinton foundation mueller rod rosenstein maybe even james comey time president united states certainly eric holder head doj knew evidence russians infiltrated purpose criminal enterprise corner market uranium foundational material nuclear weapons asked hannitytoensing said correct via cns news\n",
      "Predicted Label: 1\n",
      "Top Features: ['russian', 'uranium one', 'sale', 'bribery', 'uranium']\n"
     ]
    }
   ],
   "source": [
    "real_explanation = explain_prediction(real_example['text'])\n",
    "fake_explanation = explain_prediction(fake_example['text'])\n",
    "\n",
    "print(\"Real Example Prediction:\")\n",
    "print(\"Text:\", real_example['text'])\n",
    "print(\"Predicted Label:\", real_explanation['label'])\n",
    "print(\"Top Features:\", real_explanation['features'])\n",
    "\n",
    "print(\"\\nFake Example Prediction:\")\n",
    "print(\"Text:\", fake_example['text'])\n",
    "print(\"Predicted Label:\", fake_explanation['label'])\n",
    "print(\"Top Features:\", fake_explanation['features'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3d2ed",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25072982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.975\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       103\n",
      "           1       0.99      0.96      0.97        97\n",
      "\n",
      "    accuracy                           0.97       200\n",
      "   macro avg       0.98      0.97      0.97       200\n",
      "weighted avg       0.98      0.97      0.97       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SVM\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"SVM Classification Report:\\n\", classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fa257",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88ed1993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.995\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       103\n",
      "           1       1.00      0.99      0.99        97\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       1.00      0.99      0.99       200\n",
      "weighted avg       1.00      0.99      0.99       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5ae4f",
   "metadata": {},
   "source": [
    "## Phase 2: Multi class classification\n",
    "One of the key challenges in this phase is to build the custom dataset. For this we are looking to scrape 1200 articles, 200 for each of the 7 categories, and clean that data. For the prototype we will manually scrape some articles from The Onion, a well known satire site. The articles scraped are the ones featured on the 2024 \"Annual Year\" post found here: https://theonion.com/our-annual-year-2024/ - the top 5 from each month have been chosen, so a total of 55 articles as December stats are not yet available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bae0a77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_onion_article(url):\n",
    "    \"\"\"\n",
    "    Scrapes an article from a given URL on theonion.com and extracts relevant information.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    url : str\n",
    "        The URL of the article to scrape.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the extracted article data.\n",
    "    \"\"\"\n",
    "    article_data = {\n",
    "        \"title\": \"\",\n",
    "        \"text\": \"\",\n",
    "        \"site\": \"\",\n",
    "        \"date\": \"\",\n",
    "        \"category\": \"\",\n",
    "        \"class\": \"Satire\", #satire is hardcoded here as we know TheOnion is a satire site\n",
    "        \"url\": url\n",
    "    }\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the title from the meta property \"og:title\"\n",
    "        title_meta = soup.find('meta', property='og:title')\n",
    "        article_data[\"title\"] = title_meta['content'] if title_meta else \"Title not found\"\n",
    "        \n",
    "        # Extract the article text from the meta property \"og:description\"\n",
    "        description_meta = soup.find('meta', property='og:description')\n",
    "        article_data[\"text\"] = description_meta['content'] if description_meta else \"Description not found\"\n",
    "        \n",
    "        # Extract the URL from the meta property \"og:url\"\n",
    "        url_meta = soup.find('meta', property='og:url')\n",
    "        article_data[\"url\"] = url_meta['content'] if url_meta else url  # Fallback to input URL\n",
    "        \n",
    "        # Extract the site name from the meta property \"og:site_name\"\n",
    "        site_name_meta = soup.find('meta', property='og:site_name')\n",
    "        article_data[\"site\"] = site_name_meta['content'] if site_name_meta else \"Site name not found\"\n",
    "        \n",
    "        # Extract the published date from the meta property \"article:published_time\"\n",
    "        published_date_meta = soup.find('meta', property='article:published_time')\n",
    "        article_data[\"date\"] = published_date_meta['content'] if published_date_meta else \"Published date not found\"\n",
    "        \n",
    "        # Extract the category (e.g., \"Politics\")\n",
    "        category_element = soup.find('div', class_='taxonomy-category')\n",
    "        category_link = category_element.find('a') if category_element else None\n",
    "        article_data[\"category\"] = category_link.text.strip() if category_link else \"Category not found\"\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to fetch the webpage: {url}. Status code: {response.status_code}\")\n",
    "    \n",
    "    return article_data\n",
    "\n",
    "\n",
    "def scrape_multiple_onion_articles(urls):\n",
    "    \"\"\"\n",
    "    Scrapes multiple articles from a list of URLs and stores the data in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    urls : list\n",
    "        A list of article URLs to scrape.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the scraped data from all URLs.\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    for url in urls:\n",
    "        article = scrape_onion_article(url)\n",
    "        articles.append(article)\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    #January\n",
    "    \"https://theonion.com/biden-addresses-nation-while-hanging-from-branch-on-sid-1851106795/\",\n",
    "    \"https://theonion.com/marriage-counselor-sides-with-hotter-spouse-1851143488/\",\n",
    "    \"https://theonion.com/wealthy-dad-surprises-child-with-tree-house-he-can-airb-1851112919/\",\n",
    "    \"https://theonion.com/glowing-pulsating-hair-product-takes-control-of-gavin-1851160421/\",\n",
    "    \"https://theonion.com/gen-z-announces-julie-andrews-is-problematic-but-refuse-1851180352/\",\n",
    "    #February\n",
    "    \"https://theonion.com/mrbeast-announces-he-has-resurrected-everyone-buried-at-1851217565/\",\n",
    "    \"https://theonion.com/introverted-cowboy-struggling-to-round-up-posse-1851226175/\",\n",
    "    \"https://theonion.com/country-stations-refuse-to-play-beyonce-s-music-after-a-1851261135/\",\n",
    "    \"https://theonion.com/stab-him-stab-him-you-cowards-says-terrified-kamal-1851243467/\",\n",
    "    \"https://theonion.com/emerging-filmmaker-malia-obama-changes-surname-to-scors-1851278946/\",\n",
    "    #March\n",
    "    \"https://theonion.com/u-s-airdrops-rubble-into-gaza-1851305713/\",\n",
    "    \"https://theonion.com/ozempic-maker-triumphantly-announces-new-drug-that-make-1851320436/\",\n",
    "    \"https://theonion.com/study-millennial-women-forgoing-dating-apps-in-favor-o-1851338275/\",\n",
    "    \"https://theonion.com/beyonce-reveals-new-country-album-cover-featuring-tooth-1851355991/\",\n",
    "    \"https://theonion.com/but-dog-likes-fighting-for-money-1851352386/\",\n",
    "    #April\n",
    "    \"https://theonion.com/finance-whiz-has-over-300-in-bank-account-1851375065/\",\n",
    "    \"https://theonion.com/sotheby-s-announces-auction-of-napkin-on-which-jeffrey-1851375213/\",\n",
    "    \"https://theonion.com/o-j-simpson-allowed-to-remain-living-after-coffin-does-1851403804/\",\n",
    "    \"https://theonion.com/travis-kelce-impresses-coachella-crowd-by-tossing-taylo-1851410856/\",\n",
    "    \"https://theonion.com/biden-carried-away-by-ants-1851422363/\",\n",
    "    #May\n",
    "    \"https://theonion.com/tesla-lays-off-entire-team-behind-brakes-1851449223/\",\n",
    "    \"https://theonion.com/drake-drops-new-track-inviting-kendrick-lamar-out-to-co-1851458534/\",\n",
    "    \"https://theonion.com/perdue-announces-initiative-to-even-the-playing-field-b-1851423157/\",\n",
    "    \"https://theonion.com/new-florida-law-requires-all-women-to-produce-3-healthy-1851482288/\",\n",
    "    \"https://theonion.com/everyone-in-er-bit-off-finger-while-holding-sandwich-1851488798/\",\n",
    "    #June\n",
    "    \"https://theonion.com/cult-leader-not-even-charismatic-1851512851/\",\n",
    "    \"https://theonion.com/embarrassed-david-attenborough-realizes-he-spent-10-min-1851512951/\",\n",
    "    \"https://theonion.com/newest-u-s-aid-mission-just-single-powerbar-labeled-f-1851540802/\",\n",
    "    \"https://theonion.com/report-every-place-on-earth-has-wrong-amount-of-water-1851544516/\",\n",
    "    \"https://theonion.com/nasa-warns-space-hawk-has-swooped-in-and-picked-up-eart-1851544578/\",\n",
    "    #July\n",
    "    \"https://theonion.com/clarence-thomas-torn-over-case-where-both-sides-offer-c-1851566812/\",\n",
    "    \"https://theonion.com/democrats-panic-after-kamala-harris-ages-40-years-in-si-1851601473/\",\n",
    "    \"https://theonion.com/congress-bans-roofs-1851592883/\",\n",
    "    \"https://theonion.com/news-happening-faster-than-man-can-generate-uninformed-1851601466/\",\n",
    "    \"https://theonion.com/god-forced-to-shave-head-after-contracting-plague-of-li-1851580149/\",\n",
    "    #August\n",
    "    \"https://theonion.com/environmentalists-warn-u-s-running-out-of-small-wooded-1851609190/\",\n",
    "    \"https://theonion.com/r-kelly-petitions-supreme-court-to-watch-him-pee-1851619802rev1723482404693/\",\n",
    "    \"https://theonion.com/federated-union-of-bear-cub-carcass-dumpers-endorses-rf-1851613425/\",\n",
    "    \"https://theonion.com/glen-powell-opens-up-about-dangerous-stunt-work-filming-with-sydney-sweeneys-breasts/\",\n",
    "    \"https://theonion.com/j-d-vance-accuses-tim-walz-of-stolen-valor-for-wearing-1851621120/\",\n",
    "    #September\n",
    "    \"https://theonion.com/everyone-in-restaurant-jealous-of-toddler-who-gets-to-wear-pajamas-and-watch-ipad/\",\n",
    "    \"https://theonion.com/horrified-taylor-swift-realizes-football-happens-every-year/\",\n",
    "    \"https://theonion.com/trump-avoids-answering-hard-questions-by-pretending-he-shot-in-ear-again/\",\n",
    "    \"https://theonion.com/man-replies-stop-to-political-fundraiser-text-like-powerful-wizard-casting-spell-to-ward-off-mythical-beast/\",\n",
    "    \"https://theonion.com/scarecrow-has-double-ds/\",\n",
    "    #October\n",
    "    \"https://theonion.com/the-onion-officially-endorses-joe-biden-for-president/\",\n",
    "    \"https://theonion.com/texas-sex-ed-class-teaches-boys-how-to-cheat-on-pregnant-wife/\",\n",
    "    \"https://theonion.com/sabrina-carpenter-completes-mandatory-service-in-south-korean-military/\",\n",
    "    \"https://theonion.com/north-carolina-family-informed-their-insurance-policy-voided-once-house-gets-wet/\",\n",
    "    \"https://theonion.com/grandma-who-survived-great-depression-casually-drops-that-she-once-killed-man-for-mayonnaise/\",\n",
    "    #November\n",
    "    \"https://theonion.com/piss-soaked-tucker-carlson-claims-demon-urinated-on-him-while-he-slept/\",\n",
    "    \"https://theonion.com/trump-calls-harris-to-congratulate-himself-on-winning/\",\n",
    "    \"https://theonion.com/america-defeats-america/\",\n",
    "    \"https://theonion.com/man-forgetting-difference-between-meteoroid-meteorite-struggles-to-describe-what-just-killed-his-dog/\",\n",
    "    \"https://theonion.com/every-movement-in-mans-burrito-eating-technique-informed-by-past-burrito-tragedies/\"\n",
    "]\n",
    "\n",
    "# Scrape articles and create a DataFrame\n",
    "custom_data_df = scrape_multiple_onion_articles(urls)\n",
    "# Store to CSV\n",
    "custom_data_df.to_csv(\"onion_scraped_articles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e42ce1",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae16ca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News             19\n",
      "Local            12\n",
      "Entertainment    12\n",
      "Politics         11\n",
      "Editorials        1\n",
      "Name: category, dtype: int64\n",
      "                                                                                               content  \\\n",
      "0  Biden Addresses Nation While Hanging From Branch On Side Of Cliff WASHINGTON—Using his platform ...   \n",
      "1  Marriage Counselor Sides With Hotter Spouse ANCHORAGE, AK—Stating that she had heard both perspe...   \n",
      "2  Wealthy Dad Surprises Child With Tree House He Can Airbnb For Passive Income WILMETTE, IL—Tellin...   \n",
      "3  Glowing, Pulsating Hair Product Takes Control Of Gavin Newsom’s Thoughts SACRAMENTO, CA—As an ot...   \n",
      "4  Gen Z Announces Julie Andrews Is Problematic But Refuses To Explain Why ​​NEW YORK—Standing befo...   \n",
      "\n",
      "    class       category  \n",
      "0  Satire       Politics  \n",
      "1  Satire          Local  \n",
      "2  Satire          Local  \n",
      "3  Satire       Politics  \n",
      "4  Satire  Entertainment  \n"
     ]
    }
   ],
   "source": [
    "#combine the title and text columns into a single column\n",
    "custom_data_df['content'] = custom_data_df['title'] + \" \" + custom_data_df['text']\n",
    "#drop unnecessary columns\n",
    "custom_data_df = custom_data_df[['content', 'class','category']] \n",
    "#check the breakdown of categories\n",
    "print(custom_data_df['category'].value_counts())\n",
    "print(custom_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb71cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                               content  \\\n",
      "0  Biden Addresses Nation While Hanging From Branch On Side Of Cliff WASHINGTON—Using his platform ...   \n",
      "1  Marriage Counselor Sides With Hotter Spouse ANCHORAGE, AK—Stating that she had heard both perspe...   \n",
      "2  Wealthy Dad Surprises Child With Tree House He Can Airbnb For Passive Income WILMETTE, IL—Tellin...   \n",
      "3  Glowing, Pulsating Hair Product Takes Control Of Gavin Newsom’s Thoughts SACRAMENTO, CA—As an ot...   \n",
      "4  Gen Z Announces Julie Andrews Is Problematic But Refuses To Explain Why ​​NEW YORK—Standing befo...   \n",
      "\n",
      "                                                                                         clean_content  \n",
      "0  biden addresses nation hanging branch side cliff washington—using platform plead americans lend ...  \n",
      "1  marriage counselor sides hotter spouse anchorage ak—stating heard perspectives could understand ...  \n",
      "2  wealthy dad surprises child tree house airbnb passive income wilmette il—telling child peek walk...  \n",
      "3  glowing pulsating hair product takes control gavin newsom’s thoughts sacramento ca—as otherworld...  \n",
      "4  gen z announces julie andrews problematic refuses explain ​​new york—standing crowd millennials ...  \n"
     ]
    }
   ],
   "source": [
    "#apply preprocessing\n",
    "custom_data_df['clean_content'] = custom_data_df['content'].apply(preprocess_text)\n",
    "print(custom_data_df[['content', 'clean_content']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddbc0b3",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b902ef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                         clean_content  \\\n",
      "0  biden addresses nation hanging branch side cliff washington—using platform plead americans lend ...   \n",
      "1  marriage counselor sides hotter spouse anchorage ak—stating heard perspectives could understand ...   \n",
      "2  wealthy dad surprises child tree house airbnb passive income wilmette il—telling child peek walk...   \n",
      "3  glowing pulsating hair product takes control gavin newsom’s thoughts sacramento ca—as otherworld...   \n",
      "4  gen z announces julie andrews problematic refuses explain ​​new york—standing crowd millennials ...   \n",
      "\n",
      "   polarity  subjectivity  \n",
      "0  0.010714      0.592857  \n",
      "1  0.190476      0.433333  \n",
      "2  0.156618      0.476471  \n",
      "3  0.105000      0.577500  \n",
      "4  0.400000      0.400000  \n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "custom_data_df[['polarity', 'subjectivity']] = custom_data_df['clean_content'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "print(custom_data_df[['clean_content','polarity', 'subjectivity']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67e4ce",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a989061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                         clean_content  \\\n",
      "0  biden addresses nation hanging branch side cliff washington—using platform plead americans lend ...   \n",
      "1  marriage counselor sides hotter spouse anchorage ak—stating heard perspectives could understand ...   \n",
      "2  wealthy dad surprises child tree house airbnb passive income wilmette il—telling child peek walk...   \n",
      "3  glowing pulsating hair product takes control gavin newsom’s thoughts sacramento ca—as otherworld...   \n",
      "4  gen z announces julie andrews problematic refuses explain ​​new york—standing crowd millennials ...   \n",
      "\n",
      "                                                                                              entities  \n",
      "0  [Biden Addresses Nation While Hanging From Branch On Side Of Cliff WASHINGTON, Americans, Joe Bi...  \n",
      "1  [Spouse ANCHORAGE, AK, Laurie Hartford, David, Julia Carter, David, at least two, half, six, her...  \n",
      "2                            [IL, Kenneth Schweitz, Tuesday, Schweitz, thousands of dollars, Schweitz]  \n",
      "3                                    [CA, California, Friday, Gavin Newsom’s, Newsom, Capitol, Newsom]  \n",
      "4  [Julie Andrews, Gen Xers, Generation Z, Wednesday, Julie Andrews, Gen Z, Taylor Collaco, million...  \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents]\n",
    "\n",
    "custom_data_df['entities'] = custom_data_df['content'].apply(extract_entities)\n",
    "print(custom_data_df[['clean_content','entities']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f900ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
