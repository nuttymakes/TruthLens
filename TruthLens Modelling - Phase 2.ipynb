{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1456b888",
   "metadata": {},
   "source": [
    "# TruthLens Modelling - Phase 2: Multi-class Classification\n",
    "The aim of phase 2 is to further classify text which has already been flagged as \"fake\" into one of four different types of fake news. These four classes - Fabricated, Polarised, Satire and Commentary - are a reduced adaption of the Molina et al. Disinformation Taxonomy.\n",
    "\n",
    "The dataset used is this phase is the custom dataset I created, which has already been cleaned and preprocessed (see \"TruthLens Data Collection\" and \"TruthLens Data Cleaning\" notebooks).\n",
    "\n",
    "### Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d80d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load a spaCy model for NER\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Assume df2 is already loaded and cleaned:\n",
    "# df2 = pd.read_csv('your_cleaned_df2.csv')\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_multiclass, y_multiclass, test_size=0.2, random_state=42)\n",
    "# Train a classifier on the training data\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the held-out test set\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Classification Report on Test Set:\\n\", classification_report(y_test, predictions))\n",
    "\n",
    "###########################\n",
    "# 1. Topic Modeling (LDA)\n",
    "###########################\n",
    "# Preprocess text for LDA: tokenize and remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    tokens = [token for token in gensim.utils.simple_preprocess(text) if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "df2['tokens'] = df2['content'].apply(preprocess)\n",
    "\n",
    "# Create a dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(df2['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df2['tokens']]\n",
    "\n",
    "# Train LDA model (e.g., 5 topics)\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "# Function to extract topic distribution for a document\n",
    "def get_topic_distribution(text):\n",
    "    bow = dictionary.doc2bow(preprocess(text))\n",
    "    # Get topic probabilities for all topics (ensure all topics are returned)\n",
    "    topic_dist = lda_model.get_document_topics(bow, minimum_probability=0.0)\n",
    "    # Return a list of probabilities ordered by topic index\n",
    "    return [prob for topic, prob in sorted(topic_dist, key=lambda x: x[0])]\n",
    "\n",
    "df2['topic_dist'] = df2['content'].apply(get_topic_distribution)\n",
    "\n",
    "###############################\n",
    "# 2. Sentiment Analysis\n",
    "###############################\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "df2[['polarity', 'subjectivity']] = df2['content'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "\n",
    "#######################################\n",
    "# 3. Named Entity Recognition (NER)\n",
    "#######################################\n",
    "def get_entity_counts(text):\n",
    "    doc = nlp(text)\n",
    "    counts = {'PERSON': 0, 'ORG': 0, 'GPE': 0}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in counts:\n",
    "            counts[ent.label_] += 1\n",
    "    return counts\n",
    "\n",
    "df2['entity_counts'] = df2['content'].apply(get_entity_counts)\n",
    "df2['person_count'] = df2['entity_counts'].apply(lambda x: x['PERSON'])\n",
    "df2['org_count'] = df2['entity_counts'].apply(lambda x: x['ORG'])\n",
    "df2['gpe_count'] = df2['entity_counts'].apply(lambda x: x['GPE'])\n",
    "\n",
    "########################################\n",
    "# 4. Domain-Specific Keyword Counts\n",
    "########################################\n",
    "# Define a dictionary for domain-specific keywords\n",
    "domain_keywords = {\n",
    "    'politics': ['election', 'government', 'senate', 'congress'],\n",
    "    'health': ['vaccine', 'covid', 'pandemic', 'healthcare'],\n",
    "    'finance': ['stock', 'market', 'economy', 'trade']\n",
    "}\n",
    "\n",
    "def count_domain_keywords(text):\n",
    "    counts = {}\n",
    "    text_lower = text.lower()\n",
    "    for category, keywords in domain_keywords.items():\n",
    "        count = sum(text_lower.count(keyword) for keyword in keywords)\n",
    "        counts[category] = count\n",
    "    return counts\n",
    "\n",
    "df2['domain_counts'] = df2['content'].apply(count_domain_keywords)\n",
    "# Expand domain keyword counts into separate columns\n",
    "domain_df = df2['domain_counts'].apply(pd.Series)\n",
    "df2 = pd.concat([df2, domain_df], axis=1)\n",
    "\n",
    "###############################################\n",
    "# 5. Combine Features into a Single Feature Vector\n",
    "###############################################\n",
    "def combine_features(row):\n",
    "    features = []\n",
    "    # Add topic distribution (list of probabilities, e.g., 5 topics)\n",
    "    features.extend(row['topic_dist'])\n",
    "    # Add sentiment scores (polarity and subjectivity)\n",
    "    features.append(row['polarity'])\n",
    "    features.append(row['subjectivity'])\n",
    "    # Add NER counts (for PERSON, ORG, GPE)\n",
    "    features.append(row['person_count'])\n",
    "    features.append(row['org_count'])\n",
    "    features.append(row['gpe_count'])\n",
    "    # Add domain-specific keyword counts (order by sorted key names)\n",
    "    for key in sorted(domain_keywords.keys()):\n",
    "        features.append(row.get(key, 0))\n",
    "    return features\n",
    "\n",
    "df2['feature_vector'] = df2.apply(combine_features, axis=1)\n",
    "\n",
    "# Create a final feature matrix (each row is a document's feature vector)\n",
    "X_multiclass = np.vstack(df2['feature_vector'].values)\n",
    "y_multiclass = df2['label']\n",
    "\n",
    "###############################################\n",
    "# Example: Training a Multi-Class Classifier\n",
    "###############################################\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_multiclass, y_multiclass)\n",
    "\n",
    "# Evaluate performance (if you have a separate validation set, use that)\n",
    "predictions = clf.predict(X_multiclass)\n",
    "print(\"Classification Report for Multi-Class Classifier:\\n\", \n",
    "      pd.Series(predictions).value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bee9fd",
   "metadata": {},
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43391e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Full Classification Report:\\n\", classification_report(y_multiclass, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
