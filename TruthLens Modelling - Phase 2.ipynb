{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1456b888",
      "metadata": {
        "id": "1456b888"
      },
      "source": [
        "# TruthLens Modelling - Phase 2: Multi-class Classification\n",
        "The aim of phase 2 is to further classify text which has already been flagged as \"fake\" into one of four different types of fake news. These four classes - Fabricated, Polarised, Satire and Commentary - are a reduced adaption of the Molina et al. Disinformation Taxonomy.\n",
        "\n",
        "The dataset used is this phase is the custom dataset I created, which has already been cleaned and preprocessed (see \"TruthLens Data Collection\" and \"TruthLens Data Cleaning\" notebooks).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0"
      ],
      "metadata": {
        "id": "-MvdLC_PkY4a",
        "outputId": "c79767ea-4bcf-4d32-ae67-89976390768d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "-MvdLC_PkY4a",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.13.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.41.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "orbax-checkpoint 0.11.10 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.6 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "017bedf1a99a4f7391f60563b4d38591"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib\n",
        "!pip install xgboost\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "bjrcCUpSoVtE",
        "outputId": "8420fcdf-d0bd-4e9f-bb9b-b4aba34df131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bjrcCUpSoVtE",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aee53cf6",
      "metadata": {
        "id": "aee53cf6"
      },
      "outputs": [],
      "source": [
        "#required imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "import time\n",
        "from itertools import chain, combinations\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "import joblib\n",
        "\n",
        "#set a seed value for reproducability\n",
        "np.random.seed(999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c5fed05f",
      "metadata": {
        "id": "c5fed05f",
        "outputId": "0ee06071-8da2-4dda-d696-183f081bd0ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "#spaCy's small English model download\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c5b6c7ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5b6c7ca",
        "outputId": "467ac653-e84b-4f8e-9ac4-589ea105fee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             content  label  word_count  \\\n",
            "0  Perdue Announces Initiative To Even The Playin...      2         207   \n",
            "1  Met Police just BLOCKED a pro-Palestine protes...      1         591   \n",
            "2  Here's the moment Mark Zuckerberg gave away th...      1         515   \n",
            "\n",
            "   sentence_count  flesch_reading_ease  \\\n",
            "0               8                45.19   \n",
            "1              22                35.91   \n",
            "2              25                50.67   \n",
            "\n",
            "                                       content_lemma  \\\n",
            "0  Perdue Announces Initiative To Even The Playin...   \n",
            "1  Met Police just BLOCKED a pro-Palestine protes...   \n",
            "2  Here 's the moment Mark Zuckerberg give away t...   \n",
            "\n",
            "                                content_lemma_nostop  \n",
            "0  perdue announces initiative even playing field...  \n",
            "1  met police blocked propalestine protest march ...  \n",
            "2  moment mark zuckerberg give away game like res...  \n",
            "--------------------------------------------------\n",
            "Class distribution:\n",
            "label\n",
            "2    400\n",
            "1    400\n",
            "3    400\n",
            "0    400\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "--------------------------------------------------\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600 entries, 0 to 1599\n",
            "Data columns (total 7 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   content               1600 non-null   object \n",
            " 1   label                 1600 non-null   int64  \n",
            " 2   word_count            1600 non-null   int64  \n",
            " 3   sentence_count        1600 non-null   int64  \n",
            " 4   flesch_reading_ease   1600 non-null   float64\n",
            " 5   content_lemma         1600 non-null   object \n",
            " 6   content_lemma_nostop  1600 non-null   object \n",
            "dtypes: float64(1), int64(3), object(3)\n",
            "memory usage: 87.6+ KB\n",
            "None \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "df = pd.read_csv('Data/phase2_final_clean.csv')\n",
        "df = df.reset_index(drop=True)\n",
        "print(df.head(3))\n",
        "print(\"-\" * 50)\n",
        "print(\"Class distribution:\")\n",
        "print(df['label'].value_counts(), \"\\n\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Dataset Information:\")\n",
        "print(df.info(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7db2d491",
      "metadata": {
        "id": "7db2d491"
      },
      "outputs": [],
      "source": [
        "X = df['content_lemma']\n",
        "y = df['label']\n",
        "\n",
        "#Stratified train-test split helps maintain class balance\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=999)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5664bd82",
      "metadata": {
        "id": "5664bd82"
      },
      "source": [
        "### Generate baseline\n",
        "We will generate a simple baseline using Logistic Regression and TF-IDF features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c568bb9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c568bb9",
        "outputId": "77ca369e-7b46-4e97-f970-ac1392bd4de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Macro F1 Score: 0.8677662531487172\n",
            "\n",
            "Classification Report for Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.89      0.81       320\n",
            "           1       0.85      0.73      0.78       320\n",
            "           2       0.94      0.87      0.90       320\n",
            "           3       0.96      0.99      0.98       320\n",
            "\n",
            "    accuracy                           0.87      1280\n",
            "   macro avg       0.87      0.87      0.87      1280\n",
            "weighted avg       0.87      0.87      0.87      1280\n",
            "\n",
            "Run time: 24.5621 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "#pipeline - creates TF-IDF features then creates the logistic regression model\n",
        "baseline_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, random_state=999))\n",
        "])\n",
        "\n",
        "#stratified k-fold cross-validation - this ensures each fold has a similar class distribution\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=999)\n",
        "\n",
        "#generate predictions\n",
        "predicted_labels = cross_val_predict(baseline_pipeline, X_train, y_train, cv=skf, method=\"predict\")\n",
        "\n",
        "#calculate f1 score\n",
        "f1_macro = f1_score(y_train, predicted_labels, average=\"macro\")\n",
        "print(\"Logistic Regression Macro F1 Score:\", f1_macro)\n",
        "\n",
        "#get classification report\n",
        "report = classification_report(y_train, predicted_labels)\n",
        "print(\"\\nClassification Report for Logistic Regression:\\n\", report)\n",
        "\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b800541",
      "metadata": {
        "id": "6b800541"
      },
      "source": [
        "### Choose best model\n",
        "Next we will test three different models to see which performs the best.\n",
        "\n",
        "#### Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b5e049da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5e049da",
        "outputId": "29af3aab-a333-4757-a858-21726f211ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Naive Bayes Macro F1 Score: 0.6480536690632447\n",
            "\n",
            "Classification Report for Multinomial Naive Bayes:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.90      0.63       320\n",
            "           1       0.68      0.62      0.65       320\n",
            "           2       0.82      0.63      0.71       320\n",
            "           3       1.00      0.43      0.60       320\n",
            "\n",
            "    accuracy                           0.65      1280\n",
            "   macro avg       0.74      0.65      0.65      1280\n",
            "weighted avg       0.74      0.65      0.65      1280\n",
            "\n",
            "Run time: 2.2450 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "mnb_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "#generate predictions\n",
        "predicted_labels_mnb = cross_val_predict(mnb_pipeline, X_train, y_train, cv=skf, method=\"predict\")\n",
        "\n",
        "#print results\n",
        "f1_macro_mnb = f1_score(y_train, predicted_labels_mnb, average=\"macro\")\n",
        "print(\"Multinomial Naive Bayes Macro F1 Score:\", f1_macro_mnb)\n",
        "print(\"\\nClassification Report for Multinomial Naive Bayes:\\n\", classification_report(y_train, predicted_labels_mnb))\n",
        "\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4375eb",
      "metadata": {
        "id": "7d4375eb"
      },
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "57660239",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57660239",
        "outputId": "fcb0513e-65ec-4f2e-dd9c-f4bf42d412bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Macro F1 Score: 0.8695792933725124\n",
            "\n",
            "Classification Report for XGBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82       320\n",
            "           1       0.84      0.74      0.79       320\n",
            "           2       0.87      0.91      0.89       320\n",
            "           3       0.98      0.97      0.98       320\n",
            "\n",
            "    accuracy                           0.87      1280\n",
            "   macro avg       0.87      0.87      0.87      1280\n",
            "weighted avg       0.87      0.87      0.87      1280\n",
            "\n",
            "Run time: 28.8425 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "xgb_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"clf\", XGBClassifier(eval_metric='mlogloss', random_state=999))\n",
        "])\n",
        "\n",
        "#generate predictions\n",
        "predicted_labels_xgb = cross_val_predict(xgb_pipeline, X_train, y_train, cv=skf, method=\"predict\")\n",
        "\n",
        "#print results\n",
        "f1_macro_xgb = f1_score(y_train, predicted_labels_xgb, average=\"macro\")\n",
        "print(\"XGBoost Macro F1 Score:\", f1_macro_xgb)\n",
        "print(\"\\nClassification Report for XGBoost:\\n\", classification_report(y_train, predicted_labels_xgb))\n",
        "\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bec846b",
      "metadata": {
        "id": "5bec846b"
      },
      "source": [
        "#### Feed forward neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "881014ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "881014ce",
        "outputId": "a5811562-c13e-4f9c-e100-fe86f39c56f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2a6d5be68d8c>:24: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  (\"clf\", KerasClassifier(build_fn=lambda: create_ffnn_model(5000),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 1ms/step\n",
            "Feedforward Neural Network Macro F1 Score: 0.855922267668467\n",
            "\n",
            "Classification Report for Feedforward Neural Network:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.87      0.78       320\n",
            "           1       0.81      0.73      0.77       320\n",
            "           2       0.95      0.83      0.89       320\n",
            "           3       0.98      0.99      0.99       320\n",
            "\n",
            "    accuracy                           0.85      1280\n",
            "   macro avg       0.86      0.85      0.86      1280\n",
            "weighted avg       0.86      0.85      0.86      1280\n",
            "\n",
            "Run time: 9.4377 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "#transformer to convert a sparse matrix (TFIDF) to a dense array because neural networks need dense arrays\n",
        "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        return X.todense()\n",
        "\n",
        "#feed forward neural network\n",
        "def create_ffnn_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"to_dense\", DenseTransformer()),\n",
        "    (\"clf\", KerasClassifier(build_fn=lambda: create_ffnn_model(5000),\n",
        "                              epochs=5, batch_size=32, verbose=0))\n",
        "])\n",
        "\n",
        "#generate predictions\n",
        "predicted_labels_nn = cross_val_predict(nn_pipeline, X_train, y_train, cv=skf, method=\"predict\")\n",
        "\n",
        "#print results\n",
        "f1_macro_nn = f1_score(y_train, predicted_labels_nn, average=\"macro\")\n",
        "print(\"Feedforward Neural Network Macro F1 Score:\", f1_macro_nn)\n",
        "print(\"\\nClassification Report for Feedforward Neural Network:\\n\", classification_report(y_train, predicted_labels_nn))\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion\n",
        "The baseline macro F1 score of 0.86 with Logistic Regression set the bar very high. Class 3 is the strongest class, while classes 0 and 1 are a bit behind with F1 scores of 0.79 and 0.77 respectively, but still well above our success metric of 0.6 for each class.\n",
        "\n",
        "The worst performing model by far was Multinomial Naive Bayes with a macro F1 score of 0.62. The inconsistency between precision and recall for different classes - class 0 has high recall but low precision while class 3 is the opposite - suggests that the naive assumption that each feature is independent of other features doesn't hold well here.\n",
        "\n",
        "The feedforward neural network is competitive with the others with a macro F1 of 0.85, but given that it shows no real advantage, and the dataset is relatively small with only 1,600 lines, the added complexity of a neural network isn't justified.\n",
        "\n",
        "XGBoost had the best macro score of 0.87 with a pretty balanced performance in each class. However, the runtime is significantly higher than the other models tested, coming in over 100 seconds while the others were all below 23 seconds.\n",
        "\n",
        "For the next phase I will take both Logistic Regression and XGBoost and experiment with some feature engineering to see if I can get improvements in the weaker classes (0 and 1)."
      ],
      "metadata": {
        "id": "2IvlT9KVoFlf"
      },
      "id": "2IvlT9KVoFlf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Engineering\n",
        "\n",
        "####Generate Features\n",
        "Next we will define some custom tramsforms to generate different types of features.\n",
        "- Sentiment transformer which computes the polarity and subjectivity of a row using TextBlob\n",
        "- Named entity recognition (NER) transformer which counts the different entity types e.g. PERSON, ORG, DATE\n",
        "- Topic modelling using gensim's LDA"
      ],
      "metadata": {
        "id": "EaxubMwk9Gl7"
      },
      "id": "EaxubMwk9Gl7"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "682273cd",
      "metadata": {
        "id": "682273cd"
      },
      "outputs": [],
      "source": [
        "#sentiment analysis\n",
        "class SentimentTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        #both polarity and subjectivity should be returned as features\n",
        "        features = np.array([\n",
        "            [TextBlob(text).sentiment.polarity, TextBlob(text).sentiment.subjectivity]\n",
        "            for text in X\n",
        "        ])\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4a8ee932",
      "metadata": {
        "id": "4a8ee932"
      },
      "outputs": [],
      "source": [
        "#named entity recognition\n",
        "class NERTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        features = []\n",
        "        for text in X:\n",
        "            doc = nlp(text)\n",
        "            counts = {}\n",
        "            for ent in doc.ents:\n",
        "                counts[ent.label_] = counts.get(ent.label_, 0) + 1\n",
        "            #we want to return the count of the different entities\n",
        "            features.append(counts)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#topic modelling\n",
        "class TopicModelingTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, num_topics=5, passes=10):\n",
        "        self.num_topics = num_topics\n",
        "        self.passes = passes\n",
        "        self.dictionary = None\n",
        "        self.lda_model = None\n",
        "    def fit(self, X, y=None):\n",
        "        #text has already been cleaned so we can split on whitespace here to save time\n",
        "        tokenized = [text.split() for text in X]\n",
        "        self.dictionary = corpora.Dictionary(tokenized)\n",
        "        corpus = [self.dictionary.doc2bow(tokens) for tokens in tokenized]\n",
        "        self.lda_model = LdaModel(corpus, num_topics=self.num_topics, id2word=self.dictionary, passes=self.passes)\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        tokenized = [text.split() for text in X]\n",
        "        corpus = [self.dictionary.doc2bow(tokens) for tokens in tokenized]\n",
        "        features = []\n",
        "        for bow in corpus:\n",
        "            #get topic distribution\n",
        "            doc_topics = self.lda_model.get_document_topics(bow, minimum_probability=0)\n",
        "            #Form a fixed-length vector\n",
        "            doc_vector = [prob for (_, prob) in sorted(doc_topics, key=lambda x: x[0])]\n",
        "            features.append(doc_vector)\n",
        "        return np.array(features)"
      ],
      "metadata": {
        "id": "C2FpDtJk85YJ"
      },
      "id": "C2FpDtJk85YJ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the transformers ready, we will create a set of models with different combinations of the features for testing."
      ],
      "metadata": {
        "id": "pkvZLiZD_G_Q"
      },
      "id": "pkvZLiZD_G_Q"
    },
    {
      "cell_type": "code",
      "source": [
        "#this is our base transformer - just TF-IDF as we used in the initial model testing\n",
        "tfidf_transformer = (\"tfidf\", Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000))\n",
        "]))\n",
        "\n",
        "#these are the other possible features that we can add\n",
        "feature_components = {\n",
        "    \"sentiment\": (\"sentiment\", SentimentTransformer()),\n",
        "    \"ner\": (\"ner\", Pipeline([\n",
        "        (\"ner\", NERTransformer()),\n",
        "        (\"vect\", DictVectorizer())\n",
        "    ])),\n",
        "    \"topic\": (\"topic\", TopicModelingTransformer(num_topics=5, passes=10))\n",
        "}\n",
        "additional_features = list(feature_components.keys())\n",
        "\n",
        "#this function is going to generate all subsets to make it easier to test different variations of features\n",
        "def powerset(iterable):\n",
        "    s = list(iterable)\n",
        "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
        "\n",
        "\n",
        "#these are the two models we chose to test\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=999),\n",
        "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=999)\n",
        "}\n",
        "\n",
        "#again we'll use stratified k-fold for evaluation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "32emQntt88o2"
      },
      "id": "32emQntt88o2",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name, clf in models.items():\n",
        "    results[model_name] = {}\n",
        "    print(f\"\\nEvaluating model: {model_name}\")\n",
        "    #iterate over all combos of additional features (including TF-IDF only)\n",
        "    for extra in powerset(additional_features):\n",
        "        #start with TF-IDF\n",
        "        components = [tfidf_transformer]\n",
        "        for feat in extra:\n",
        "            components.append(feature_components[feat])\n",
        "        #join the features horizontally\n",
        "        union = FeatureUnion(components)\n",
        "        #define pipeline\n",
        "        pipeline = Pipeline([\n",
        "            (\"features\", union),\n",
        "            (\"clf\", clf)\n",
        "        ])\n",
        "        #evaluate\n",
        "        predicted = cross_val_predict(pipeline, X_train, y_train, cv=skf, method=\"predict\")\n",
        "        f1_macro = f1_score(y_train, predicted, average=\"macro\")\n",
        "\n",
        "        #create a key for this combo\n",
        "        key = \"tfidf\"\n",
        "        if extra:\n",
        "            key += \"+\" + \"+\".join(extra)\n",
        "        else:\n",
        "            key += \"_only\"\n",
        "\n",
        "        results[model_name][key] = f1_macro\n",
        "        print(f\"Features: {key:30s} | Macro F1: {f1_macro:.4f}\")\n",
        "\n",
        "\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "id": "9vZV56t_A-DF",
        "outputId": "3ee6b191-ac96-4ac6-ad1e-1c3ffe473311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9vZV56t_A-DF",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model: LogisticRegression\n",
            "Features: tfidf_only                     | Macro F1: 0.8643\n",
            "Features: tfidf+sentiment                | Macro F1: 0.8567\n",
            "Features: tfidf+ner                      | Macro F1: 0.8468\n",
            "Features: tfidf+topic                    | Macro F1: 0.8613\n",
            "Features: tfidf+sentiment+ner            | Macro F1: 0.8451\n",
            "Features: tfidf+sentiment+topic          | Macro F1: 0.8476\n",
            "Features: tfidf+ner+topic                | Macro F1: 0.8475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: tfidf+sentiment+ner+topic      | Macro F1: 0.8468\n",
            "\n",
            "Evaluating model: XGBoost\n",
            "Features: tfidf_only                     | Macro F1: 0.8612\n",
            "Features: tfidf+sentiment                | Macro F1: 0.8657\n",
            "Features: tfidf+ner                      | Macro F1: 0.8702\n",
            "Features: tfidf+topic                    | Macro F1: 0.8604\n",
            "Features: tfidf+sentiment+ner            | Macro F1: 0.8702\n",
            "Features: tfidf+sentiment+topic          | Macro F1: 0.8690\n",
            "Features: tfidf+ner+topic                | Macro F1: 0.8714\n",
            "Features: tfidf+sentiment+ner+topic      | Macro F1: 0.8608\n",
            "Run time: 7957.8377 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion\n",
        "The best combination is the XGBoost model with TF-IDF, named entity recognition and topic modelling, with a macro F1 score of 0.88. This is the model that we will use for phase 2. Finally we will do a simple grid search to see if we can further tune the model.\n",
        "\n",
        "#### Build model"
      ],
      "metadata": {
        "id": "VPkAP_QWiqVM"
      },
      "id": "VPkAP_QWiqVM"
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "#create NER pipeline, use DictVectorizer to convert dicts to numeric features\n",
        "ner_pipeline = Pipeline([\n",
        "    (\"ner\", NERTransformer()),\n",
        "    (\"vect\", DictVectorizer())\n",
        "])\n",
        "\n",
        "#create topic modelling pipeline\n",
        "topic_pipeline = Pipeline([\n",
        "    (\"topic\", TopicModelingTransformer(num_topics=5, passes=10))\n",
        "])\n",
        "\n",
        "#combine features using FeatureUnion\n",
        "combined_features = FeatureUnion([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"ner\", ner_pipeline),\n",
        "    (\"topic\", topic_pipeline)\n",
        "])\n",
        "\n",
        "#build the final pipeline with XGBoost\n",
        "final_pipeline = Pipeline([\n",
        "    (\"features\", combined_features),\n",
        "    (\"clf\", XGBClassifier(eval_metric='mlogloss', random_state=999))\n",
        "])\n",
        "\n",
        "#split data into training and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=999)\n",
        "\n",
        "#fit model\n",
        "final_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate on the test set\n",
        "y_pred = final_pipeline.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Macro F1 Score:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
        "\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa8wLaPIio04",
        "outputId": "bd22c8f8-c748-43ab-ff00-dd587fdd838d"
      },
      "id": "wa8wLaPIio04",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.79      0.82        80\n",
            "           1       0.79      0.80      0.80        80\n",
            "           2       0.88      0.94      0.91        80\n",
            "           3       1.00      1.00      1.00        80\n",
            "\n",
            "    accuracy                           0.88       320\n",
            "   macro avg       0.88      0.88      0.88       320\n",
            "weighted avg       0.88      0.88      0.88       320\n",
            "\n",
            "Macro F1 Score: 0.8805759457933371\n",
            "Run time: 168.8624 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "joblib.dump(final_pipeline, \"final_pipeline.pkl\")\n",
        "print(\"Model saved to final_pipeline.pkl\")"
      ],
      "metadata": {
        "id": "KByxpv4LqS4Q",
        "outputId": "eb2cdc65-9952-4baf-dd30-bcca53e8ec80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KByxpv4LqS4Q",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to final_pipeline.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Grid Search"
      ],
      "metadata": {
        "id": "S7E7PqiTph9J"
      },
      "id": "S7E7PqiTph9J"
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "param_grid = {\n",
        "    \"clf__max_depth\": [3, 5, 7],\n",
        "    \"clf__learning_rate\": [0.01, 0.1],\n",
        "    \"clf__subsample\": [0.8, 1.0]\n",
        "}\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=999)\n",
        "grid_search = GridSearchCV(estimator=final_pipeline, param_grid=param_grid, scoring=\"f1_macro\", cv=skf, n_jobs=1, verbose=1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Macro F1 Score (CV):\", grid_search.best_score_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "print(\"\\nTest Set Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "print(\"Test Set Macro F1 Score:\", f1_score(y_test, y_pred_test, average=\"macro\"))\n",
        "\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ],
      "metadata": {
        "id": "Ngr1ONpxB95V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2556ed0c-7a77-4e74-b412-926b89fe88e0"
      },
      "id": "Ngr1ONpxB95V",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__subsample': 0.8}\n",
            "Best Macro F1 Score (CV): 0.8717698590876699\n",
            "\n",
            "Test Set Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87        80\n",
            "           1       0.91      0.78      0.84        80\n",
            "           2       0.88      0.95      0.92        80\n",
            "           3       0.99      1.00      0.99        80\n",
            "\n",
            "    accuracy                           0.91       320\n",
            "   macro avg       0.91      0.91      0.91       320\n",
            "weighted avg       0.91      0.91      0.91       320\n",
            "\n",
            "Test Set Macro F1 Score: 0.9050041452608242\n",
            "Run time: 4937.4634 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(best_model, \"phase_2_final_model_pipeline.pkl\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "ba4clXqto5WF",
        "outputId": "b2d9aa90-8873-496c-f4c8-b1df9d32e9c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ba4clXqto5WF",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-8RsiHVXH6SE"
      },
      "id": "-8RsiHVXH6SE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}