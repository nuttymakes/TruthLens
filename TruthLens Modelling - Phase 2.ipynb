{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1456b888",
      "metadata": {
        "id": "1456b888"
      },
      "source": [
        "# TruthLens Modelling - Phase 2: Multi-class Classification\n",
        "The aim of phase 2 is to further classify text which has already been flagged as \"fake\" into one of four different types of fake news. These four classes - Fabricated, Polarised, Satire and Commentary - are a reduced adaption of the Molina et al. Disinformation Taxonomy.\n",
        "\n",
        "The dataset used is this phase is the custom dataset I created, which has already been cleaned and preprocessed (see \"TruthLens Data Collection\" and \"TruthLens Data Cleaning\" notebooks).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0"
      ],
      "metadata": {
        "id": "-MvdLC_PkY4a"
      },
      "id": "-MvdLC_PkY4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "aee53cf6",
      "metadata": {
        "id": "aee53cf6"
      },
      "outputs": [],
      "source": [
        "#required imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "#set a seed value for reproducability\n",
        "np.random.seed(999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5fed05f",
      "metadata": {
        "id": "c5fed05f"
      },
      "outputs": [],
      "source": [
        "#spaCy's small English model download\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c5b6c7ca",
      "metadata": {
        "id": "c5b6c7ca",
        "outputId": "8bd1d031-4653-4bc2-ef04-8d5b7e84085b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             content  label  word_count  \\\n",
            "0  Perdue Announces Initiative To Even The Playin...      2         207   \n",
            "1  Met Police just BLOCKED a pro-Palestine protes...      1         591   \n",
            "2  Here's the moment Mark Zuckerberg gave away th...      1         515   \n",
            "\n",
            "   sentence_count  flesch_reading_ease  \\\n",
            "0               8                45.19   \n",
            "1              22                35.91   \n",
            "2              25                50.67   \n",
            "\n",
            "                                       content_lemma  \\\n",
            "0  Perdue Announces Initiative To Even The Playin...   \n",
            "1  Met Police just BLOCKED a pro-Palestine protes...   \n",
            "2  Here 's the moment Mark Zuckerberg give away t...   \n",
            "\n",
            "                                content_lemma_nostop  \n",
            "0  perdue announces initiative even playing field...  \n",
            "1  met police blocked propalestine protest march ...  \n",
            "2  moment mark zuckerberg give away game like res...  \n",
            "--------------------------------------------------\n",
            "Class distribution:\n",
            "label\n",
            "2    400\n",
            "1    400\n",
            "3    400\n",
            "0    400\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "--------------------------------------------------\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600 entries, 0 to 1599\n",
            "Data columns (total 7 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   content               1600 non-null   object \n",
            " 1   label                 1600 non-null   int64  \n",
            " 2   word_count            1600 non-null   int64  \n",
            " 3   sentence_count        1600 non-null   int64  \n",
            " 4   flesch_reading_ease   1600 non-null   float64\n",
            " 5   content_lemma         1600 non-null   object \n",
            " 6   content_lemma_nostop  1600 non-null   object \n",
            "dtypes: float64(1), int64(3), object(3)\n",
            "memory usage: 87.6+ KB\n",
            "None \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "df = pd.read_csv('Data/phase2_final_clean.csv')\n",
        "df = df.reset_index(drop=True)\n",
        "print(df.head(3))\n",
        "print(\"-\" * 50)\n",
        "print(\"Class distribution:\")\n",
        "print(df['label'].value_counts(), \"\\n\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Dataset Information:\")\n",
        "print(df.info(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7db2d491",
      "metadata": {
        "id": "7db2d491"
      },
      "outputs": [],
      "source": [
        "X = df['content_lemma']\n",
        "y = df['label']\n",
        "\n",
        "#Stratified train-test split helps maintain class balance\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5664bd82",
      "metadata": {
        "id": "5664bd82"
      },
      "source": [
        "### Generate baseline\n",
        "We will generate a simple baseline using Logistic Regression and TF-IDF features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9c568bb9",
      "metadata": {
        "id": "9c568bb9",
        "outputId": "44e18826-a883-40c9-fae1-bad8a126cad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Macro F1 Score: 0.858466261722639\n",
            "\n",
            "Classification Report for Logistic Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.86      0.79       320\n",
            "           1       0.83      0.72      0.77       320\n",
            "           2       0.92      0.87      0.89       320\n",
            "           3       0.96      0.98      0.97       320\n",
            "\n",
            "    accuracy                           0.86      1280\n",
            "   macro avg       0.86      0.86      0.86      1280\n",
            "weighted avg       0.86      0.86      0.86      1280\n",
            "\n",
            "Run time: 16.1295 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "#pipeline - creates TF-IDF features then creates the logistic regression model\n",
        "baseline_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "#stratified k-fold cross-validation - this ensures each fold has a similar class distribution\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "#generate predictions\n",
        "predicted_labels = cross_val_predict(baseline_pipeline, X_train, y_train, cv=skf, method=\"predict\")\n",
        "\n",
        "#calculate f1 score\n",
        "f1_macro = f1_score(y_train, predicted_labels, average=\"macro\")\n",
        "print(\"Logistic Regression Macro F1 Score:\", f1_macro)\n",
        "\n",
        "#get classification report\n",
        "report = classification_report(y_train, predicted_labels)\n",
        "print(\"\\nClassification Report for Logistic Regression:\\n\", report)\n",
        "\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b800541",
      "metadata": {
        "id": "6b800541"
      },
      "source": [
        "### Choose best model\n",
        "Next we will test three different models to see which performs the best.\n",
        "\n",
        "#### Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b5e049da",
      "metadata": {
        "id": "b5e049da",
        "outputId": "0afd2c25-5a72-45bf-c9b2-93a5652d9107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Naive Bayes Macro F1 Score: 0.6195257542552863\n",
            "\n",
            "Classification Report for Multinomial Naive Bayes:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.88      0.62       320\n",
            "           1       0.61      0.59      0.60       320\n",
            "           2       0.82      0.64      0.72       320\n",
            "           3       1.00      0.37      0.54       320\n",
            "\n",
            "    accuracy                           0.62      1280\n",
            "   macro avg       0.73      0.62      0.62      1280\n",
            "weighted avg       0.73      0.62      0.62      1280\n",
            "\n",
            "Run time: 9.1392 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "mnb_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "#generate predictions\n",
        "predicted_labels_mnb = cross_val_predict(mnb_pipeline, X_train, y_train, cv=skf, method=\"predict\")\n",
        "\n",
        "#print results\n",
        "f1_macro_mnb = f1_score(y_train, predicted_labels_mnb, average=\"macro\")\n",
        "print(\"Multinomial Naive Bayes Macro F1 Score:\", f1_macro_mnb)\n",
        "print(\"\\nClassification Report for Multinomial Naive Bayes:\\n\", classification_report(y_train, predicted_labels_mnb))\n",
        "\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d4375eb",
      "metadata": {
        "id": "7d4375eb"
      },
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "57660239",
      "metadata": {
        "id": "57660239",
        "outputId": "f7249589-e1aa-43d2-d517-7d7707a138c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Macro F1 Score: 0.8723158805931358\n",
            "\n",
            "Classification Report for XGBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82       320\n",
            "           1       0.82      0.75      0.78       320\n",
            "           2       0.89      0.91      0.90       320\n",
            "           3       0.99      0.98      0.99       320\n",
            "\n",
            "    accuracy                           0.87      1280\n",
            "   macro avg       0.87      0.87      0.87      1280\n",
            "weighted avg       0.87      0.87      0.87      1280\n",
            "\n",
            "Run time: 125.7894 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "xgb_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"clf\", XGBClassifier(eval_metric='mlogloss', random_state=42))\n",
        "])\n",
        "\n",
        "#generate predictions\n",
        "predicted_labels_xgb = cross_val_predict(xgb_pipeline, X_train, y_train, cv=skf, method=\"predict\")\n",
        "\n",
        "#print results\n",
        "f1_macro_xgb = f1_score(y_train, predicted_labels_xgb, average=\"macro\")\n",
        "print(\"XGBoost Macro F1 Score:\", f1_macro_xgb)\n",
        "print(\"\\nClassification Report for XGBoost:\\n\", classification_report(y_train, predicted_labels_xgb))\n",
        "\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bec846b",
      "metadata": {
        "id": "5bec846b"
      },
      "source": [
        "#### Feed forward neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "881014ce",
      "metadata": {
        "id": "881014ce",
        "outputId": "0ddf4356-95b4-410d-d8e4-dd72a4701bfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-3d5e9544c52b>:25: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  (\"clf\", KerasClassifier(build_fn=lambda: create_ffnn_model(5000),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Feedforward Neural Network Macro F1 Score: 0.8497909187063192\n",
            "\n",
            "Classification Report for Feedforward Neural Network:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.87      0.78       320\n",
            "           1       0.83      0.70      0.76       320\n",
            "           2       0.90      0.85      0.88       320\n",
            "           3       0.99      0.98      0.99       320\n",
            "\n",
            "    accuracy                           0.85      1280\n",
            "   macro avg       0.86      0.85      0.85      1280\n",
            "weighted avg       0.86      0.85      0.85      1280\n",
            "\n",
            "Run time: 16.0026 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "\n",
        "#transformer to convert a sparse matrix (TFIDF) to a dense array because neural networks need dense arrays\n",
        "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        return X.todense()\n",
        "\n",
        "#feed forward neural network\n",
        "def create_ffnn_model(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "nn_pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"to_dense\", DenseTransformer()),\n",
        "    (\"clf\", KerasClassifier(build_fn=lambda: create_ffnn_model(5000),\n",
        "                              epochs=5, batch_size=32, verbose=0))\n",
        "])\n",
        "\n",
        "#generate predictions\n",
        "predicted_labels_nn = cross_val_predict(nn_pipeline, X_train, y_train, cv=skf, method=\"predict\")\n",
        "\n",
        "#print results\n",
        "f1_macro_nn = f1_score(y_train, predicted_labels_nn, average=\"macro\")\n",
        "print(\"Feedforward Neural Network Macro F1 Score:\", f1_macro_nn)\n",
        "print(\"\\nClassification Report for Feedforward Neural Network:\\n\", classification_report(y_train, predicted_labels_nn))\n",
        "print(\"Run time: {:.4f} seconds\".format(time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion\n",
        "The baseline macro F1 score of 0.86 with Logistic Regression set the bar very high. Class 3 is the strongest class, while classes 0 and 1 are a bit behind with F1 scores of 0.79 and 0.77 respectively, but still well above our success metric of 0.6 for each class.\n",
        "\n",
        "The worst performing model by far was Multinomial Naive Bayes with a macro F1 score of 0.62. The inconsistency between precision and recall for different classes - class 0 has high recall but low precision while class 3 is the opposite - suggests that the naive assumption that each feature is independent of other features doesn't hold well here.\n",
        "\n",
        "The feedforward neural network is competitive with the others with a macro F1 of 0.85, but given that it shows no real advantage, and the dataset is relatively small with only 1,600 lines, the added complexity of a neural network isn't justified.\n",
        "\n",
        "XGBoost had the best macro score of 0.87 with a pretty balanced performance in each class. However, the runtime is significantly higher than the other models tested, coming in over 100 seconds while the others were all below 23 seconds.\n",
        "\n",
        "For the next phase I will take both Logistic Regression and XGBoost and experiment with some feature engineering to see if I can get improvements in the weaker classes (0 and 1)."
      ],
      "metadata": {
        "id": "2IvlT9KVoFlf"
      },
      "id": "2IvlT9KVoFlf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682273cd",
      "metadata": {
        "id": "682273cd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8ee932",
      "metadata": {
        "id": "4a8ee932"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}